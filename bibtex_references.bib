@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}
@article{bolukbasi2016man,
  title={Man is to computer programmer as woman is to homemaker? debiasing word embeddings},
  author={Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James Y and Saligrama, Venkatesh and Kalai, Adam T},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={4349--4357},
  year={2016}
}
@article{merullo2019investigating,
  title={Investigating sports commentator bias within a large corpus of American football broadcasts},
  author={Merullo, Jack and Yeh, Luke and Handler, Abram and Grissom II, Alvin and O'Connor, Brendan and Iyyer, Mohit},
  journal={arXiv preprint arXiv:1909.03343},
  year={2019}
}
@article{hermann2014multilingual,
  title={Multilingual models for compositional distributed semantics},
  author={Hermann, Karl Moritz and Blunsom, Phil},
  journal={arXiv preprint arXiv:1404.4641},
  year={2014}
}

@article{bengio_neural_2003,
	title = {A Neural Probabilistic Language Model},
	abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difﬁcult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to ﬁght the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a signiﬁcant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach signiﬁcantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
	pages = {19},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	date = {2003},
	year = {2003},
	langid = {english},
}

@inproceedings{chen2018unsupervised,
  title={Unsupervised Multilingual Word Embeddings},
  author={Chen, Xilun and Cardie, Claire},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={261--270},
  year={2018}
}

@article{mikolov2013exploiting,
  title={Exploiting similarities among languages for machine translation},
  author={Mikolov, Tomas and Le, Quoc V and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1309.4168},
  year={2013}
}

@article{sprugnoli_vir_2019,
	title = {Vir is to Moderatus as Mulier is to Intemperans Lemma Embeddings for Latin},
	pages = {7},
	author = {Sprugnoli, Rachele and Passarotti, Marco and Moretti, Giovanni},
	date = {2019},
	year = {2019},
	langid = {english},
}

@inproceedings{herbelot_high-risk_2017,
	location = {Copenhagen, Denmark},
	title = {High-risk learning: acquiring new word vectors from tiny data},
	url = {https://aclanthology.org/D17-1030},
	doi = {10.18653/v1/D17-1030},
	shorttitle = {High-risk learning},
	abstract = {Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn `a good vector' for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences' worth of context, showing a large increase in performance over state-of-the-art models on the definitional task.},
	eventtitle = {{EMNLP} 2017},
	pages = {304--309},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Herbelot, Aurélie and Baroni, Marco},
	urldate = {2021-11-09},
	date = {2017-09},
	year =  {2019}
}

@inproceedings{papineni_bleu_2002,
	location = {Philadelphia, Pennsylvania, {USA}},
	title = {Bleu: a Method for Automatic Evaluation of Machine Translation},
	url = {https://aclanthology.org/P02-1040},
	doi = {10.3115/1073083.1073135},
	shorttitle = {Bleu},
	eventtitle = {{ACL} 2002},
	pages = {311--318},
	booktitle = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	urldate = {2021-11-08},
	date = {2002-07},
	year =  {2002}
}

@article{putnam_review_1984,
	title = {Review of The Aeneid of Virgil},
	issn = {0506-7294},
	url = {https://www.jstor.org/stable/41591898},
	pages = {64--67},
	number = {30},
	journaltitle = {Vergilius (1959-)},
	author = {Putnam, Michael C. J.},
	editora = {Fitzgerald, Robert},
	editoratype = {collaborator},
	urldate = {2021-11-04},
	date = {1984},
	note = {Publisher: The Vergilian Society},
}

@article{putnam_review_1984-1,
	title = {Review of The Aeneid of Virgil},
	issn = {0506-7294},
	url = {https://www.jstor.org/stable/41591898},
	pages = {64--67},
	number = {30},
	journaltitle = {Vergilius (1959-)},
	author = {Putnam, Michael C. J.},
	editora = {Fitzgerald, Robert},
	editoratype = {collaborator},
	urldate = {2021-11-04},
	date = {1984},
	note = {Publisher: The Vergilian Society},
}

@book{virgil_aeneid_1911,
	location = {New York},
	title = {The Æneid of Virgil},
	url = {https://catalog.hathitrust.org/Record/100639530},
	pagetotal = {333},
	publisher = {C. Scribner's sons},
	author = {{Virgil} and Ballard, Harlan H.},
	urldate = {2021-11-04},
	date = {1911},
	keywords = {Harlan Hoge Ballard},
}

@online{noauthor_aeneid_nodate,
	title = {The Æneid of Virgil, tr. by Harlan Hoge Ballard., . - Text-only - Full View {\textbar} {HathiTrust} Digital Library},
	url = {https://babel.hathitrust.org/cgi/ssd?id=uc1.b000933162;page=ssd;view=plaintext;seq=172;num=154},
	urldate = {2021-11-04},
}

@inproceedings{bloem-etal-2019-evaluating,
  title={Evaluating the consistency of word embeddings from small data},
  author={Bloem, Jelke and Fokkens, Antske and Herbelot, Aur{\'e}lie},
  booktitle={Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)},
  pages={132--141},
  year={2019}
}
@InProceedings{pmlr-v37-gouws15,
  title = 	 {BilBOWA: Fast Bilingual Distributed Representations without Word Alignments},
  author = 	 {Gouws, Stephan and Bengio, Yoshua and Corrado, Greg},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {748--756},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/gouws15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/gouws15.html},
  abstract = 	 {We introduce BilBOWA (Bilingual Bag-of-Words without Alignments), a simple and computationally-efficient model for learning bilingual distributed representations of words which can scale to large monolingual datasets and does not require word-aligned parallel training data. Instead it trains directly on monolingual data and extracts a bilingual signal from a smaller set of raw-text sentence-aligned data. This is achieved using a novel sampled bag-of-words cross-lingual objective, which is used to regularize two noise-contrastive language models for efficient cross-lingual feature learning. We show that bilingual embeddings learned using the proposed model outperforms state-of-the-art methods on a cross-lingual document classification task as well as a lexical translation task on the WMT11 data.}
}


@inproceedings{bloem_distributional_2020,
	location = {Marseille, France},
	title = {Distributional Semantics for Neo-Latin},
	isbn = {979-10-95546-53-5},
	url = {https://aclanthology.org/2020.lt4hala-1.13},
	abstract = {We address the problem of creating and evaluating quality Neo-Latin word embeddings for the purpose of philosophical research, adapting the Nonce2Vec tool to learn embeddings from Neo-Latin sentences. This distributional semantic modeling tool can learn from tiny data incrementally, using a larger background corpus for initialization. We conduct two evaluation tasks: definitional learning of Latin Wikipedia terms, and learning consistent embeddings from 18th century Neo-Latin sentences pertaining to the concept of mathematical method. Our results show that consistent Neo-Latin word embeddings can be learned from this type of data. While our evaluation results are promising, they do not reveal to what extent the learned models match domain expert knowledge of our Neo-Latin texts. Therefore, we propose an additional evaluation method, grounded in expert-annotated data, that would assess whether learned representations are conceptually sound in relation to the domain of study.},
	pages = {84--93},
	booktitle = {Proceedings of {LT}4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages},
	publisher = {European Language Resources Association ({ELRA})},
	author = {Bloem, Jelke and Parisi, Maria Chiara and Reynaert, Martin and Oortwijn, Yvette and Betti, Arianna},
	urldate = {2021-10-29},
	date = {2020-05},
	year =  {2020},
}

@inproceedings{upadhyay_cross-lingual_2016,
	location = {Berlin, Germany},
	title = {Cross-lingual Models of Word Embeddings: An Empirical Comparison},
	url = {https://aclanthology.org/P16-1157},
	doi = {10.18653/v1/P16-1157},
	shorttitle = {Cross-lingual Models of Word Embeddings},
	eventtitle = {{ACL} 2016},
	pages = {1661--1670},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Upadhyay, Shyam and Faruqui, Manaal and Dyer, Chris and Roth, Dan},
	urldate = {2021-10-23},
	date = {2016-08},
	year =  {2016}
}

@online{denooz_opera_nodate,
	title = {Opera latina: une base de données sur internet},
	url = {https://www.brepolsonline.net/doi/epdf/10.1484/J.EUPHR.5.125535},
	shorttitle = {Opera latina},
	author = {Denooz, Joseph},
	urldate = {2021-10-07},
	doi = {10.1484/J.EUPHR.5.125535},
}

@article{appiah_there_2016,
	title = {There is no such thing as western civilisation},
	issn = {0261-3077},
	url = {https://www.theguardian.com/world/2016/nov/09/western-civilisation-appiah-reith-lecture},
	abstract = {The Long Read: The values of liberty, tolerance and rational inquiry are not the birthright of a single culture. In fact, the very notion of something called ‘western culture’ is a modern invention},
	journaltitle = {The Guardian},
	author = {Appiah, Kwame Anthony},
	urldate = {2021-10-07},
	date = {2016-11-09},
	langid = {british},
	keywords = {Philosophy, World news},
}

@inproceedings{levy_dependency-based_2014,
	location = {Baltimore, Maryland},
	title = {Dependency-Based Word Embeddings},
	url = {https://aclanthology.org/P14-2050},
	doi = {10.3115/v1/P14-2050},
	eventtitle = {{ACL} 2014},
	pages = {302--308},
	booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Levy, Omer and Goldberg, Yoav},
	urldate = {2021-10-07},
	date = {2014-06},
	year = {2014},
}

@inproceedings{pennington_glove_2014,
	location = {Doha, Qatar},
	title = {{GloVe}: Global Vectors for Word Representation},
	url = {https://aclanthology.org/D14-1162},
	doi = {10.3115/v1/D14-1162},
	shorttitle = {{GloVe}},
	eventtitle = {{EMNLP} 2014},
	pages = {1532--1543},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	urldate = {2021-10-07},
	date = {2014-10},
	year =  {2014},
}

@inproceedings{mikolov_linguistic_2013,
	location = {Atlanta, Georgia},
	title = {Linguistic Regularities in Continuous Space Word Representations},
	url = {https://aclanthology.org/N13-1090},
	eventtitle = {{NAACL}-{HLT} 2013},
	pages = {746--751},
	booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	urldate = {2021-10-07},
	date = {2013-06},
	year =  {2013},
}

@book{osgood_measurement_1957,
	title = {The Measurement of Meaning},
	isbn = {978-0-252-74539-3},
	abstract = {In this pioneering study, the authors  deal with the nature and theory of meaning and present a new, objective  method for its measurement which they call the semantic differential.  This instrument is not a specific test, but rather a general technique of  measurement that can be adapted to a wide variety of problems in such areas  as clinical psychology, social psychology, linguistics, mass communications,  esthetics, and political science. The core of the book is the authors' description,  application, and evaluation of this important tool and its far-reaching  implications for empirical research.},
	pagetotal = {358},
	publisher = {University of Illinois Press},
	author = {Osgood, Charles Egerton and Suci, George J. and Tannenbaum, Percy H.},
	date = {1957},
	langid = {english},
	note = {Google-Books-{ID}: Qj8GeUrKZdAC},
	keywords = {Psychology / General},
}

@article{harris_distributional_1954,
	title = {Distributional Structure},
	volume = {10},
	issn = {0043-7956, 2373-5112},
	url = {http://www.tandfonline.com/doi/full/10.1080/00437956.1954.11659520},
	doi = {10.1080/00437956.1954.11659520},
	pages = {146--162},
	number = {2},
	journaltitle = {\textit{{WORD}}},
	shortjournal = {\textit{{WORD}}},
	author = {Harris, Zellig S.},
	urldate = {2021-10-07},
	date = {1954-08},
	langid = {english},
}

@article{bamman_extracting_2012,
	title = {Extracting two thousand years of latin from a million book library},
	volume = {5},
	issn = {1556-4673, 1556-4711},
	url = {https://dl.acm.org/doi/10.1145/2160165.2160167},
	doi = {10.1145/2160165.2160167},
	abstract = {With the rise of large open digitization projects such as the Internet Archive and Google Books, we are witnessing an explosive growth in the number of source texts becoming available to researchers in historical languages. The Internet Archive alone contains over 27,014 texts catalogued as Latin, including classical prose and poetry written under the Roman Empire, ecclesiastical treatises from the Middle Ages, and dissertations from 19th-century Germany written—in Latin—on the philosophy of Hegel. At one billion words, this collection eclipses the extant corpus of Classical Latin by several orders of magnitude. In addition, the much larger collection of books in English, German, French, and other languages already scanned contains unknown numbers of translations for many Latin books, or parts of books.
            
              The sheer scale of this collection offers a broad vista of new research questions, and we focus here on both the opportunities and challenges of computing over such a large space of heterogeneous texts. The works in this massive collection do not constitute a finely curated (or much less balanced) corpus of Latin; it is, instead, simply
              all
              the Latin that can be extracted, and in its reach of twenty-one centuries (from approximately 200 {BCE} to 1922 {CE}) arguably spans the greatest historical distance of any major textual collection today. While we might hope that the size and historical reach of this collection can eventually offer insight into grand questions such as the evolution of a language over both time and space, we must contend as well with the noise inherent in a corpus that has been assembled with minimal human intervention.},
	pages = {1--13},
	number = {1},
	journaltitle = {Journal on Computing and Cultural Heritage},
	shortjournal = {J. Comput. Cult. Herit.},
	author = {Bamman, David and Smith, David},
	urldate = {2021-10-05},
	date = {2012-04},
	langid = {english},
}

@inproceedings{bamman_measuring_2011,
	location = {Ottawa, Ontario, Canada},
	title = {Measuring historical word sense variation},
	isbn = {978-1-4503-0744-4},
	url = {http://portal.acm.org/citation.cfm?doid=1998076.1998078},
	doi = {10.1145/1998076.1998078},
	abstract = {We describe here a method for automatically identifying word sense variation in a dated collection of historical books in a large digital library. By leveraging a small set of known translation book pairs to induce a bilingual sense inventory and labeled training data for a {WSD} classiﬁer, we are able to automatically classify the Latin word senses in a 389 million word corpus and track the rise and fall of those senses over a span of two thousand years. We evaluate the performance of seven diﬀerent classiﬁers both in a tenfold test on 83,892 words from the aligned parallel corpus and on a smaller, manually annotated sample of 525 words, measuring both the overall accuracy of each system and how well that accuracy correlates (via mean square error) to the observed historical variation.},
	eventtitle = {Proceeding of the 11th annual international {ACM}/{IEEE} joint conference},
	pages = {1},
	booktitle = {Proceeding of the 11th annual international {ACM}/{IEEE} joint conference on Digital libraries - {JCDL} '11},
	publisher = {{ACM} Press},
	author = {Bamman, David and Crane, Gregory},
	urldate = {2021-10-05},
	date = {2011},
	langid = {english},
}

@article{franzini_towards_nodate,
	title = {Towards an Evaluation of the Latin {WordNet}},
	pages = {8},
	author = {Franzini, Greta and Peverelli, Andrea and Ruffolo, Paolo and Passarotti, Marco and Sanna, Helena and Signoroni, Edoardo and Ventura, Viviana and Zampedri, Federica},
	langid = {english},
}

@inproceedings{bjerva_word_2015,
	location = {Beijing, China},
	title = {Word Embeddings Pointing the Way for Late Antiquity},
	url = {https://aclanthology.org/W15-3708},
	doi = {10.18653/v1/W15-3708},
	pages = {53--57},
	booktitle = {Proceedings of the 9th {SIGHUM} Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities ({LaTeCH})},
	publisher = {Association for Computational Linguistics},
	author = {Bjerva, Johannes and Praet, Raf},
	urldate = {2021-10-05},
	date = {2015-07},
}

@inproceedings{burns_profiling_2021,
	location = {Online},
	title = {Profiling of Intertextuality in Latin Literature Using Word Embeddings},
	url = {https://aclanthology.org/2021.naacl-main.389},
	doi = {10.18653/v1/2021.naacl-main.389},
	abstract = {Identifying intertextual relationships between authors is of central importance to the study of literature. We report an empirical analysis of intertextuality in classical Latin literature using word embedding models. To enable quantitative evaluation of intertextual search methods, we curate a new dataset of 945 known parallels drawn from traditional scholarship on Latin epic poetry. We train an optimized word2vec model on a large corpus of lemmatized Latin, which achieves state-of-the-art performance for synonym detection and outperforms a widely used lexical method for intertextual search. We then demonstrate that training embeddings on very small corpora can capture salient aspects of literary style and apply this approach to replicate a previous intertextual study of the Roman historian Livy, which relied on hand-crafted stylometric features. Our results advance the development of core computational resources for a major premodern language and highlight a productive avenue for cross-disciplinary collaboration between the study of literature and {NLP}.},
	eventtitle = {{NAACL}-{HLT} 2021},
	pages = {4900--4907},
	booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	publisher = {Association for Computational Linguistics},
	author = {Burns, Patrick J. and Brofos, James A. and Li, Kyle and Chaudhuri, Pramit and Dexter, Joseph P.},
	urldate = {2021-10-05},
	date = {2021-06},
}

@inproceedings{hamilton_diachronic_2016,
	location = {Berlin, Germany},
	title = {Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change},
	url = {https://aclanthology.org/P16-1141},
	doi = {10.18653/v1/P16-1141},
	eventtitle = {{ACL} 2016},
	pages = {1489--1501},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Hamilton, William L. and Leskovec, Jure and Jurafsky, Dan},
	urldate = {2021-10-04},
	date = {2016-08},
}

@article{bojanowski_enriching_2017,
	title = {Enriching Word Vectors with Subword Information},
	volume = {5},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00051},
	doi = {10.1162/tacl_a_00051},
	pages = {135--146},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	urldate = {2021-10-05},
	year = {2017},
}

@article{ruder_survey_2019,
	title = {A Survey of Cross-lingual Word Embedding Models},
	volume = {65},
	issn = {1076-9757},
	doi = {10.1613/jair.1.11640},
	abstract = {Cross-lingual representations of words enable us to reason about word meaning in multilingual contexts and are a key facilitator of cross-lingual transfer when developing natural language processing models for low-resource languages. In this survey, we provide a comprehensive typology of cross-lingual word embedding models. We compare their data requirements and objective functions. The recurring theme of the survey is that many of the models presented in the literature optimize for the same objectives, and that seemingly different models are often equivalent, modulo optimization strategies, hyper-parameters, and such. We also discuss the different ways cross-lingual word embeddings are evaluated, as well as future challenges and research horizons.},
	pages = {569--631},
	journaltitle = {The Journal of artificial intelligence research},
	author = {Ruder, Sebastian and Vulić, Ivan and Søgaard, Anders},
	date = {2019},
	keywords = {Survey of Word Embeddings},
}

@book{venuti_translation_2012,
	location = {Hoboken},
	edition = {3rd ed.},
	title = {Translation studies reader},
	isbn = {978-1-135-73204-2},
	series = {{EBL}.},
	pagetotal = {561},
	publisher = {Taylor and Francis},
	author = {Venuti, Lawrence},
	date = {2012},
	keywords = {History, Translating and interpreting, Translation},
}

@inproceedings{coulmance_trans-gram_2015,
	location = {Lisbon, Portugal},
	title = {Trans-gram, Fast Cross-lingual Word-embeddings},
	url = {http://aclweb.org/anthology/D15-1131},
	doi = {10.18653/v1/D15-1131},
	abstract = {We introduce Trans-gram, a simple and computationally-efﬁcient method to simultaneously learn and align wordembeddings for a variety of languages, using only monolingual data and a smaller set of sentence-aligned data. We use our new method to compute aligned wordembeddings for twenty-one languages using English as a pivot language. We show that some linguistic features are aligned across languages for which we do not have aligned data, even though those properties do not exist in the pivot language. We also achieve state of the art results on standard cross-lingual text classiﬁcation and word translation tasks.},
	eventtitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	pages = {1109--1113},
	booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Coulmance, Jocelyn and Marty, Jean-Marc and Wenzek, Guillaume and Benhalloum, Amine},
	urldate = {2021-09-24},
	date = {2015},
	langid = {english},
	year =  {2015},
}

@online{noauthor_semantic_nodate,
	title = {Semantic Intertextual Search with Latin Word-Embedding Models},
	url = {https://classicalstudies.org/annual-meeting/152/abstract/semantic-intertextual-search-latin-word-embedding-models},
	titleaddon = {Society for Classical Studies},
	type = {Text},
	urldate = {2021-09-23},
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	urldate = {2021-09-23},
	date = {2013},
	keywords = {{CS} foundation},
}

@online{noauthor_now_nodate,
	title = {Now Reading.. The Aeneid {PDF} Book @{BD}9F80DE5CA9F9E03A01AADB176B5822.{SOURCESET}.{CO}.{IL}},
	url = {http://bd9f80de5ca9f9e03a01aadb176b5822.sourceset.co.il/read/291d8064ea1ddd84e9d62787b79d9520.pdf},
	urldate = {2021-09-23},
}

@online{noauthor_methods_nodate,
	title = {The Methods of Medieval Translators : A Comparison of the Latin Text of Virgil's Aeneid with Its Old French Adaptations},
	url = {https://web.a.ebscohost.com/ehost/ebookviewer/ebook/ZTAwMHhuYV9fNDU4NDYxX19BTg2?sid=f334c478-fef3-4034-baf8-649408f33d51@sessionmgr4007&vid=0&format=EB&lpid=lp_65&rid=0},
	urldate = {2021-09-23},
	keywords = {Translation Theories},
}

@book{farrell_companion_2010,
	location = {Hoboken, {UNITED} {KINGDOM}},
	title = {A Companion to Vergil's Aeneid and Its Tradition},
	isbn = {978-1-4443-1806-7},
	url = {http://ebookcentral.proquest.com/lib/haverford/detail.action?docID=514428},
	publisher = {John Wiley \& Sons, Incorporated},
	author = {Farrell, Joseph and Putnam, Michael C. J.},
	urldate = {2021-09-23},
	date = {2010},
	keywords = {Aeneas (Legendary character) in literature., Epic poetry, Latin -- History and criticism., Virgil -- Appreciation., Virgil. -- Aeneis.},
}

